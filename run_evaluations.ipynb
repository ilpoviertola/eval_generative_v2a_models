{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate generative multimodal audio models\n",
    "\n",
    "Use this notebook to evaluate the generative multimodal audio models. This notebook uses following metrics:\n",
    "\n",
    "- Frechet Audio Distance (FAD)\n",
    "    - FAD measures how similar the distributions of these synthetic and real data samples are in the feature space of a pre-trained audio classifier\n",
    "    - Smaller FAD score indicates that the generated audio is similar to the ground truth audio\n",
    "- Kulback-Leibler Divergence (KLD)\n",
    "    - KLD measures how one probability distribution diverges from a second, expected probability distribution\n",
    "    - DKL​(P∣∣Q)\\=∑i​P(i)log(Q(i)P(i)​) where P is the ground truth and Q is the generated audio\n",
    "    - DKL(P∣∣Q) measures how much information is lost when Q is used to approximate P\n",
    "    - Smaller KLD score indicates that the generated audio is similar to the ground truth audio\n",
    "- Audio-Visual Synchronisation Score\n",
    "    - This is stated as a fraction of the files that are considered to be synchronised (0: all files are unsynchronised, 1: all files are synchronised)\n",
    "- AVCLIP Score\n",
    "    - Following the ideas of ImageBind score and CLIP score: Since AVCLIP is trained to learn a joint-embedding across six distinct modalities,the cosine similarity of its embeddings for both video and generated audio can capture semantic relevance between them.\n",
    "    - Higher AVCLIP score indicates that the generated audio is similar to the ground truth audio (max 1)\n",
    "- Zero Crossing Rate (ZCR) Similarity [1]\n",
    "    - Zero Crossing Rate (ZCR) Similarity is a metric that quantifies the similarity between the zero crossing rates of the original audio and the generated audio signals. The zero crossing rate is the rate at which the audio waveform crosses the horizontal axis (zero amplitude) and changes its polarity (from positive to negative or vice versa). The ZCR can provide insights into the temporal characteristics and overall shape of the audio signal.\n",
    "    - The ZCR similarity ranges from 0 to 1, where a value of 1 indicates a perfect match between the zero crossing rates of the two audio signals. A higher ZCR similarity score suggests that the original and generated audio signals have similar patterns of changes in amplitude and share comparable temporal characteristics.\n",
    "- Rhythm Similarity [1]\n",
    "    - Rhythm Similarity is a metric that evaluates the similarity between the rhythmic patterns of the original audio and the generated audio signals. It focuses on the presence and timing of onsets, which are abrupt changes or transients in the audio waveform that often correspond to beats or rhythmic events.\n",
    "    - The rhythm similarity score ranges from 0 to 1, with a value of 1 indicating a perfect match between the rhythmic patterns of the two audio signals. A higher rhythm similarity score suggests that the original and generated audio exhibit similar rhythmic structures, including the timing and occurrence of onsets.\n",
    "    - While rhythm similarity provides valuable insights into the rhythmic patterns of audio signals, it should be complemented with other metrics and contextual information for a comprehensive assessment of audio similarity. Additionally, variations in rhythmic interpretation or different musical genres may influence the perception of rhythm similarity, requiring domain-specific considerations.\n",
    "- Spectral Contrast Similarity [1]\n",
    "    - The spectral contrast similarity is a metric that quantifies the similarity between the spectral contrast features of the original audio and the generated audio. Spectral contrast measures the difference in magnitude between peaks and valleys in the audio spectrum, providing information about the spectral richness and emphasis of different frequency regions.\n",
    "    - The spectral contrast similarity score ranges from 0 to 1, with a value of 1 indicating a perfect match in terms of spectral contrast. A higher spectral contrast similarity score suggests that the original and generated audio exhibit similar patterns of spectral emphasis and richness, indicating similarities in the distribution of energy across different frequency bands.\n",
    "    - It's important to note that spectral contrast similarity primarily focuses on the spectral richness and emphasis patterns and may not capture other aspects of audio similarity, such as temporal dynamics or pitch content. Therefore, it is recommended to combine spectral contrast similarity with other metrics and perceptual evaluations to obtain a more comprehensive understanding of audio similarity, especially in cases where temporal or melodic differences may impact the perceived similarity.\n",
    "\n",
    "[1] M. Stent, Audio-Similarity, (2023), GitHub repository (https://github.com/markstent/audio-similarity)\n",
    "\n",
    "❗️**Note**❗️ Sometimes GPU resources are not freed until the Jupyter Kernel is restarted. If you encounter CUDA out of memory error, please restart the kernel and run the notebook again.\n",
    "\n",
    "## Setup\n",
    "User must have\n",
    "\n",
    "1. Videos with generated audio\n",
    "2. Ground truth videos\n",
    "3. Initialised the environment according to [README.md](README.md)\n",
    "\n",
    "## Helpers and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from eval_utils.utils import dataclass_from_dict\n",
    "from configs.evaluation_cfg import EvaluationCfg, PipelineCfg\n",
    "from metrics.evaluation_metrics import EvaluationMetrics\n",
    "from metrics.evaluation_metrics_combiner import EvaluationMetricsCombiner\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_config(eval_cfg_dict: dict):\n",
    "    evaluation_cfg = dataclass_from_dict(EvaluationCfg, eval_cfg_dict)\n",
    "    assert type(evaluation_cfg) == EvaluationCfg\n",
    "    assert type(evaluation_cfg.pipeline) == PipelineCfg\n",
    "    return evaluation_cfg\n",
    "\n",
    "\n",
    "def get_calculated_evaluation_metrics(\n",
    "    evaluation_cfg: EvaluationCfg, force_recalculate: bool = False\n",
    "):\n",
    "    print(f\"Evaluating ({evaluation_cfg.id}):\", evaluation_cfg.sample_directory.as_posix())\n",
    "    evaluation_metrics = EvaluationMetrics(evaluation_cfg)\n",
    "    assert type(evaluation_metrics) == EvaluationMetrics\n",
    "    evaluation_metrics.run_all(force_recalculate)\n",
    "    evaluation_metrics.export_results()\n",
    "    print(\"Evaluation done\\n\")\n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define configurations\n",
    "\n",
    "<p align=\"center\">❗️<b>NOTE</b>❗️</p>\n",
    "<p align=\"center\">Only modify the following cell with your arguments and paths. <span style=\"color:red\">Do not touch any other cell if not stated otherwise.</span></p>\n",
    "<p align=\"center\">❗️<b>NOTE</b>❗️</p>\n",
    "\n",
    "1. Define the IDs and paths to the videos with model-generated audio (*ids_sample_dirs*)\n",
    "2. Define the path to ground truth videos (*gt_dir*)\n",
    "3. Define the evaluation pipeline (*pipeline_cfg_dict*)\n",
    "    - Define the metrics to be used (fad, kld, insync)\n",
    "    - Define the parameters for individual metrics (see ./configs/evaluation_cfg.py for more details)\n",
    "    - Example:\n",
    "        - Only insync metric with default params: {\"insync\": {}}\n",
    "        - All the metrics with default params: {\"fad\": {}, \"kld\": {}, \"insync\": {}}\n",
    "        - Only FAD calculated using PCA: {\"fad\": {\"use_pca\": True}}\n",
    "4. Define verbosity (*is_verbose*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_sample_dirs = [\n",
    "    (\"gh-syncsonix-flattened-best\", \"/home/hdd/ilpo/logs/synchronisonix/24-04-17T15-58-35/checkpoints/generated_samples_24-04-18T10-03-37\"),\n",
    "    (\"gh-syncsonix-unflattened\", \"/home/hdd/ilpo/checkpoints/synchronisonix/24-02-27T16-46-55/24-02-27T16-46-55/generated_samples_24-04-17T14-24-06\"),\n",
    "    (\"gh-specvqgan\", \"/home/ilpo/repos/SpecVQGAN/logs/2024-04-16T14-16-42_greatesthit_transformer/samples_2024-04-17T09-54-50/GreatestHits_test/videos/partition1\"),\n",
    "]\n",
    "gt_dir = \"/home/hdd/ilpo/datasets/greatesthit/vis-data-256_h264_video_25fps_256side_24000hz_aac_len_5_splitby_random\"\n",
    "metadata = \"./data/vggsound_sparse.csv\"\n",
    "pipeline_cfg_dict = {\"fad\": {}, \"kld\": {}, \"insync\": {}, \"avclip_score\": {}}\n",
    "is_verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dir = Path(gt_dir)\n",
    "metadata = Path(metadata)\n",
    "ids_sample_dirs = [(id, Path(p)) for id, p in ids_sample_dirs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pipeline_cfg_dict is not None, \"Pipeline is not defined or it is empty.\"\n",
    "evaluation_cfgs = [\n",
    "    get_evaluation_config(\n",
    "        {\n",
    "            \"id\": id,\n",
    "            \"sample_directory\": sample_dir,\n",
    "            \"gt_directory\": gt_dir,\n",
    "            \"metadata\": metadata,\n",
    "            \"pipeline\": pipeline_cfg_dict,\n",
    "            \"verbose\": is_verbose,\n",
    "        }\n",
    "    )\n",
    "    for (id, sample_dir) in ids_sample_dirs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "Metrics class are initialised with the *EvaluationCfg* -class which defines the evaluation pipeline. The class is used to calculate the metrics for a single sample directory (EvaluationCfg entry). *EvaluationMetricsCombiner* -class is used to combine the metrics for all the sample directories for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    get_calculated_evaluation_metrics(evaluation_cfg)\n",
    "    for evaluation_cfg in evaluation_cfgs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = EvaluationMetricsCombiner(metrics)\n",
    "pprint(combined_results.combine())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "Plotting the combined results. **Here you can define the plotting directory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for the plots (if desired)\n",
    "# if not defined, the plots will not be saved but returned as matplotlib figures and\n",
    "# displayed in the notebook. (You can save them manually from the notebook)\n",
    "plot_dir = \".\"\n",
    "combined_results.plot(plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in metrics:\n",
    "    m.remove_resampled_directories()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval_gen_audio_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
